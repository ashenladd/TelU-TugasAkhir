{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1PwzKTSqLUEKMqTPnII6D3GkHHd1Xtfba\n",
      "From (redirected): https://drive.google.com/uc?id=1PwzKTSqLUEKMqTPnII6D3GkHHd1Xtfba&confirm=t&uuid=c28cad7a-3a36-40e1-a452-b039e45ed026\n",
      "To: /workspace/TelU-TugasAkhir/lung_image_sets.zip\n",
      "100%|████████████████████████████████████████| 929M/929M [00:44<00:00, 21.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Mendownload dataset\n",
    "!gdown 1PwzKTSqLUEKMqTPnII6D3GkHHd1Xtfba\n",
    "\n",
    "#Melakukan unzip file dataset\n",
    "!unzip -q lung_image_sets.zip\n",
    "\n",
    "#Menghapus file zip\n",
    "!rm -rf lung_image_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import shutil   \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/TelU-TugasAkhir'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "CWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lung_scc', 'lung_n', 'lung_aca']\n"
     ]
    }
   ],
   "source": [
    "extracted_folder_path = os.path.join(CWD, 'lung_image_sets')\n",
    "source_files = os.listdir(extracted_folder_path)\n",
    "print(source_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(CWD, 'lung_image_sets', 'train')\n",
    "val_dir = os.path.join(CWD, 'lung_image_sets', 'validation')\n",
    "test_dir = os.path.join(CWD, 'lung_image_sets', 'test')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "def split_data(source_dir, train_dir, val_dir, test_dir, train_ratio=0.72, val_ratio=0.2, test_ratio=0.08):\n",
    "    # Get all the files in the source directory\n",
    "    file_names = os.listdir(source_dir)\n",
    "    \n",
    "    # Split data into train and remaining (val + test)\n",
    "    train_files, temp_files = train_test_split(file_names, test_size=(1 - train_ratio))\n",
    "    \n",
    "    # Now split the remaining data into validation and test sets\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=(test_ratio / (val_ratio + test_ratio)))\n",
    "    \n",
    "    # Move files to respective directories\n",
    "    for file_name in train_files:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(train_dir, file_name))\n",
    "        \n",
    "    for file_name in val_files:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(val_dir, file_name))\n",
    "        \n",
    "    for file_name in test_files:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(test_dir, file_name))\n",
    "\n",
    "    print(\"Data split into train, validation, and test sets.\")\n",
    "\n",
    "split_data(extracted_folder_path, train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_generators(train_dir, val_dir, test_dir, target_size=(224, 224), batch_size=32, preprocessing_func=None):\n",
    "    # ImageDataGenerator without augmentation (for comparison)\n",
    "    train_datagen_no_aug = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "    )\n",
    "\n",
    "    # ImageDataGenerator with augmentation\n",
    "    train_datagen_with_aug = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=preprocessing_func\n",
    "    )\n",
    "    \n",
    "    # For validation and test sets, we do not apply augmentation\n",
    "    val_test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        preprocessing_function=preprocessing_func\n",
    "    )\n",
    "    \n",
    "    # Flow images from directories\n",
    "    train_generator_no_aug = train_datagen_no_aug.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    train_generator_with_aug = train_datagen_with_aug.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    val_generator = val_test_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    test_generator = val_test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator_no_aug, train_generator_with_aug, val_generator, test_generator\n",
    "\n",
    "# Pilih fungsi preprocessing yang sesuai dengan model\n",
    "preprocess_input_func = efficientnet_preprocess\n",
    "\n",
    "train_generator_no_aug, train_generator_with_aug, val_generator, test_generator = create_image_generators(\n",
    "    train_dir=train_dir,\n",
    "    val_dir=val_dir,\n",
    "    test_dir=test_dir,\n",
    "    target_size=(224, 224),  # Image size\n",
    "    batch_size=32,           # Batch size\n",
    "    preprocessing_func=preprocess_input_func  # Optional preprocessing function (e.g., for VGG16 or other models)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(base_model_class, input_shape):\n",
    "    base_model = base_model_class(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Tidak perlu preprocessing lagi di sini\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "efficientnet_b0 = create_model(EfficientNetB0, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model_name, checkpoint_path, patience=5, reduce_lr_factor=0.5, reduce_lr_patience=5):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "\n",
    "    saving_path = os.path.join(checkpoint_path, f'model_{model_name}.h5')\n",
    "\n",
    "    # ModelCheckpoint to save the best model based on validation accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=saving_path,  # Path to save the model\n",
    "        monitor='val_accuracy',    # Monitor validation accuracy\n",
    "        save_best_only=True,       # Save the model only when val_accuracy improves\n",
    "        mode='max',                # Maximize validation accuracy\n",
    "        verbose=1                  # Show messages when saving\n",
    "    )\n",
    "    \n",
    "    # EarlyStopping to stop training when the model stops improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',        # Monitor validation loss\n",
    "        patience=patience,         # Stop after 'patience' epochs without improvement\n",
    "        mode='min',                # Minimize validation loss\n",
    "        restore_best_weights=True, # Restore the best weights after stopping\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # ReduceLROnPlateau to reduce the learning rate when a plateau is detected\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',        # Monitor validation loss\n",
    "        factor=reduce_lr_factor,   # Factor by which to reduce learning rate\n",
    "        patience=reduce_lr_patience, # How many epochs to wait before reducing\n",
    "        mode='min',                # Minimize validation loss\n",
    "        min_lr=1e-6,               # Lower bound on the learning rate\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Return the list of callbacks\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "# Define the path to save the best model\n",
    "checkpoint_path = CWD + '/model'\n",
    "\n",
    "# Create the callbacks\n",
    "callbacks_eb0_aug = create_callbacks(\n",
    "    model_name='EfficientNetB0_aug',\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    patience=5,       \n",
    ")\n",
    "\n",
    "callbacks_eb0_no_aug = create_callbacks(\n",
    "    model_name='EfficientNetB0_no_aug',\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    patience=5,       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk melatih model dan menampilkan grafik\n",
    "def train_and_plot(model, model_name, train_generator, validation_generator, callbacks, epochs=20):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks= callbacks\n",
    "    )\n",
    "\n",
    "    # Plotting hasil\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(efficientnet_b0, 'EfficientNetB0_aug', train_generator_no_aug, val_generator, callbacks_eb0_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_and_plot(efficientnet_b0, 'EfficientNetB0_no_aug', train_generator_with_aug, val_generator, callbacks_eb0_no_aug)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
