{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1PwzKTSqLUEKMqTPnII6D3GkHHd1Xtfba\n",
      "From (redirected): https://drive.google.com/uc?id=1PwzKTSqLUEKMqTPnII6D3GkHHd1Xtfba&confirm=t&uuid=c28cad7a-3a36-40e1-a452-b039e45ed026\n",
      "To: /workspace/TelU-TugasAkhir/lung_image_sets.zip\n",
      "100%|████████████████████████████████████████| 929M/929M [00:44<00:00, 21.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Mendownload dataset\n",
    "!gdown 1PwzKTSqLUEKMqTPnII6D3GkHHd1Xtfba\n",
    "\n",
    "#Melakukan unzip file dataset\n",
    "!unzip -q lung_image_sets.zip\n",
    "\n",
    "#Menghapus file zip\n",
    "!rm -rf lung_image_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import shutil   \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/TelU-TugasAkhir'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "CWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lung_scc', 'lung_n', 'lung_aca']\n"
     ]
    }
   ],
   "source": [
    "extracted_folder_path = os.path.join(CWD, 'lung_image_sets')\n",
    "source_files = os.listdir(extracted_folder_path)\n",
    "print(source_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.2857142857142857 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mmove(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(source_dir, file_name), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_dir, file_name))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData split into train, validation, and test sets.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(source_dir, train_dir, val_dir, test_dir, train_ratio, val_ratio, test_ratio)\u001b[0m\n\u001b[1;32m     15\u001b[0m train_files, temp_files \u001b[38;5;241m=\u001b[39m train_test_split(file_names, test_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m train_ratio))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Now split the remaining data into validation and test sets\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m val_files, test_files \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Move files to respective directories\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m train_files:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:2785\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:2415\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2412\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2419\u001b[0m     )\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2857142857142857 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(CWD, 'lung_image_sets', 'train')\n",
    "val_dir = os.path.join(CWD, 'lung_image_sets', 'validation')\n",
    "test_dir = os.path.join(CWD, 'lung_image_sets', 'test')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "def split_data(source_dir, train_dir, val_dir, test_dir, train_ratio=0.72, val_ratio=0.2, test_ratio=0.08):\n",
    "    # Get all the files in the source directory\n",
    "    file_names = os.listdir(source_dir)\n",
    "    \n",
    "    # Split data into train and remaining (val + test)\n",
    "    train_files, temp_files = train_test_split(file_names, test_size=(1 - train_ratio))\n",
    "    \n",
    "    # Now split the remaining data into validation and test sets\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=(test_ratio / (val_ratio + test_ratio)))\n",
    "    \n",
    "    # Move files to respective directories\n",
    "    for file_name in train_files:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(train_dir, file_name))\n",
    "        \n",
    "    for file_name in val_files:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(val_dir, file_name))\n",
    "        \n",
    "    for file_name in test_files:\n",
    "        shutil.move(os.path.join(source_dir, file_name), os.path.join(test_dir, file_name))\n",
    "\n",
    "    print(\"Data split into train, validation, and test sets.\")\n",
    "\n",
    "split_data(source_files, train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_generators(train_dir, val_dir, test_dir, target_size=(224, 224), batch_size=32, preprocessing_func=None):\n",
    "    # ImageDataGenerator without augmentation (for comparison)\n",
    "    train_datagen_no_aug = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "    )\n",
    "\n",
    "    # ImageDataGenerator with augmentation\n",
    "    train_datagen_with_aug = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=preprocessing_func\n",
    "    )\n",
    "    \n",
    "    # For validation and test sets, we do not apply augmentation\n",
    "    val_test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        preprocessing_function=preprocessing_func\n",
    "    )\n",
    "    \n",
    "    # Flow images from directories\n",
    "    train_generator_no_aug = train_datagen_no_aug.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    train_generator_with_aug = train_datagen_with_aug.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    val_generator = val_test_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    test_generator = val_test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator_no_aug, train_generator_with_aug, val_generator, test_generator\n",
    "\n",
    "# Pilih fungsi preprocessing yang sesuai dengan model\n",
    "preprocess_input_func = efficientnet_preprocess\n",
    "\n",
    "train_generator_no_aug, train_generator_with_aug, val_generator, test_generator = create_image_generators(\n",
    "    train_dir=train_dir,\n",
    "    val_dir=val_dir,\n",
    "    test_dir=test_dir,\n",
    "    target_size=(224, 224),  # Image size\n",
    "    batch_size=32,           # Batch size\n",
    "    preprocessing_func=preprocess_input_func  # Optional preprocessing function (e.g., for VGG16 or other models)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(base_model_class, input_shape):\n",
    "    base_model = base_model_class(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Tidak perlu preprocessing lagi di sini\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "efficientnet_b0 = create_model(EfficientNetB0, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model_name, checkpoint_path, patience=5, reduce_lr_factor=0.5, reduce_lr_patience=5):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "\n",
    "    saving_path = os.path.join(checkpoint_path, f'model_{model_name}.h5')\n",
    "\n",
    "    # ModelCheckpoint to save the best model based on validation accuracy\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=saving_path,  # Path to save the model\n",
    "        monitor='val_accuracy',    # Monitor validation accuracy\n",
    "        save_best_only=True,       # Save the model only when val_accuracy improves\n",
    "        mode='max',                # Maximize validation accuracy\n",
    "        verbose=1                  # Show messages when saving\n",
    "    )\n",
    "    \n",
    "    # EarlyStopping to stop training when the model stops improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',        # Monitor validation loss\n",
    "        patience=patience,         # Stop after 'patience' epochs without improvement\n",
    "        mode='min',                # Minimize validation loss\n",
    "        restore_best_weights=True, # Restore the best weights after stopping\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # ReduceLROnPlateau to reduce the learning rate when a plateau is detected\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',        # Monitor validation loss\n",
    "        factor=reduce_lr_factor,   # Factor by which to reduce learning rate\n",
    "        patience=reduce_lr_patience, # How many epochs to wait before reducing\n",
    "        mode='min',                # Minimize validation loss\n",
    "        min_lr=1e-6,               # Lower bound on the learning rate\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Return the list of callbacks\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "# Define the path to save the best model\n",
    "checkpoint_path = CWD + '/model'\n",
    "\n",
    "# Create the callbacks\n",
    "callbacks_eb0_aug = create_callbacks(\n",
    "    model_name='EfficientNetB0_aug',\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    patience=5,       \n",
    ")\n",
    "\n",
    "callbacks_eb0_no_aug = create_callbacks(\n",
    "    model_name='EfficientNetB0_no_aug',\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    patience=5,       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk melatih model dan menampilkan grafik\n",
    "def train_and_plot(model, model_name, train_generator, validation_generator, callbacks, epochs=20):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks= callbacks\n",
    "    )\n",
    "\n",
    "    # Plotting hasil\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(efficientnet_b0, 'EfficientNetB0_aug', train_generator_no_aug, val_generator, callbacks_eb0_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_and_plot(efficientnet_b0, 'EfficientNetB0_no_aug', train_generator_with_aug, val_generator, callbacks_eb0_no_aug)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
